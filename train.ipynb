{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "jpath = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_DIR = \"./pg_binary/log\"\n",
    "writer = SummaryWriter(log_dir=LOSS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./spectogram_data_splitted\"\n",
    "MODEL_NAME = \"resnet50\"\n",
    "CLASSES = ['lion','frog','cat','chicken','sheep','donkey','monkey','dog','cow','bird']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "FEATURE_EXTRACT = False\n",
    "INPUT_SIZE = (128, 1024)\n",
    "LOG_INTERVAL = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "Label 'bird' convert to id ('0')\n",
      "Label 'cat' convert to id ('1')\n",
      "Label 'chicken' convert to id ('2')\n",
      "Label 'cow' convert to id ('3')\n",
      "Label 'dog' convert to id ('4')\n",
      "Label 'donkey' convert to id ('5')\n",
      "Label 'frog' convert to id ('6')\n",
      "Label 'lion' convert to id ('7')\n",
      "Label 'monkey' convert to id ('8')\n",
      "Label 'sheep' convert to id ('9')\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    " # Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(INPUT_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(INPUT_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "modes = ['train', 'test']\n",
    "image_datasets = {mode: datasets.ImageFolder(jpath(DATA_DIR, mode), data_transforms[mode]) for mode in ['train', 'test']}\n",
    "for class_name, idx in image_datasets[\"train\"].class_to_idx.items():\n",
    "    print(f\"Label '{class_name}' convert to id ('{idx}')\")\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {mode: torch.utils.data.DataLoader(image_datasets[mode], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for mode in ['train', 'test']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_finetune import make_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = make_model(MODEL_NAME, num_classes=NUM_CLASSES, pretrained=True, input_size=INPUT_SIZE)\n",
    "model.cuda()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, _iter):\n",
    "    total_loss = 0\n",
    "    total_size = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(dataloaders_dict[\"train\"]):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        writer.add_scalar('Loss/train', loss.item(), _iter)\n",
    "        total_loss += loss.item()\n",
    "        total_size += data.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tAverage loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dataloaders_dict[\"train\"].dataset),\n",
    "                100. * batch_idx / len(dataloaders_dict[\"train\"]), total_loss / total_size))\n",
    "        _iter += 1\n",
    "    return _iter\n",
    "\n",
    "def val(_iter):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloaders_dict[\"test\"]:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(dataloaders_dict[\"test\"].dataset)\n",
    "    writer.add_scalar('Loss/val', test_loss, _iter)\n",
    "    writer.add_scalar('Accuracy/val', correct / len(dataloaders_dict[\"test\"].dataset), _iter)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(dataloaders_dict[\"test\"].dataset),\n",
    "        100. * correct / len(dataloaders_dict[\"test\"].dataset)))\n",
    "    \n",
    "def eval():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloaders_dict[\"test\"]:\n",
    "            y_true += target.data.tolist()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            y_pred += pred.squeeze(1).tolist()\n",
    "            correct += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
    "    test_loss /= len(dataloaders_dict[\"test\"].dataset)\n",
    "    writer.add_scalar('Accuracy/val', correct / len(dataloaders_dict[\"test\"].dataset), _iter)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(dataloaders_dict[\"test\"].dataset),\n",
    "        100. * correct / len(dataloaders_dict[\"test\"].dataset)))\n",
    "    target_names = [0]*NUM_CLASSES\n",
    "    correct = [i for i, j in zip(y_pred, y_true) if i == j]\n",
    "    for class_name, idx in image_datasets[\"test\"].class_to_idx.items():\n",
    "        target_names[idx] = class_name\n",
    "#         print(f\"Class/{class_name}\", correct.count(idx), y_true.count(idx))\n",
    "        writer.add_scalar(f\"Class/{class_name}\", correct.count(idx) / y_true.count(idx), _iter)\n",
    "    print(target_names)\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/691 (0%)]\tAverage loss: 0.073546\n",
      "Train Epoch: 2 [0/691 (0%)]\tAverage loss: 0.051914\n",
      "Train Epoch: 3 [0/691 (0%)]\tAverage loss: 0.035629\n",
      "Train Epoch: 4 [0/691 (0%)]\tAverage loss: 0.025949\n",
      "Train Epoch: 5 [0/691 (0%)]\tAverage loss: 0.024380\n",
      "Train Epoch: 6 [0/691 (0%)]\tAverage loss: 0.010521\n",
      "Train Epoch: 7 [0/691 (0%)]\tAverage loss: 0.008859\n",
      "Train Epoch: 8 [0/691 (0%)]\tAverage loss: 0.004936\n",
      "Train Epoch: 9 [0/691 (0%)]\tAverage loss: 0.006960\n",
      "Train Epoch: 10 [0/691 (0%)]\tAverage loss: 0.005105\n",
      "Train Epoch: 11 [0/691 (0%)]\tAverage loss: 0.003456\n",
      "Train Epoch: 12 [0/691 (0%)]\tAverage loss: 0.001597\n",
      "Train Epoch: 13 [0/691 (0%)]\tAverage loss: 0.001724\n",
      "Train Epoch: 14 [0/691 (0%)]\tAverage loss: 0.001067\n",
      "Train Epoch: 15 [0/691 (0%)]\tAverage loss: 0.001104\n",
      "Train Epoch: 16 [0/691 (0%)]\tAverage loss: 0.000732\n",
      "Train Epoch: 17 [0/691 (0%)]\tAverage loss: 0.000316\n",
      "Train Epoch: 18 [0/691 (0%)]\tAverage loss: 0.000800\n",
      "Train Epoch: 19 [0/691 (0%)]\tAverage loss: 0.000580\n",
      "Train Epoch: 20 [0/691 (0%)]\tAverage loss: 0.000361\n",
      "Train Epoch: 21 [0/691 (0%)]\tAverage loss: 0.000996\n",
      "Train Epoch: 22 [0/691 (0%)]\tAverage loss: 0.000472\n",
      "Train Epoch: 23 [0/691 (0%)]\tAverage loss: 0.000411\n",
      "Train Epoch: 24 [0/691 (0%)]\tAverage loss: 0.000151\n",
      "Train Epoch: 25 [0/691 (0%)]\tAverage loss: 0.000250\n",
      "Train Epoch: 26 [0/691 (0%)]\tAverage loss: 0.000375\n",
      "Train Epoch: 27 [0/691 (0%)]\tAverage loss: 0.000306\n",
      "Train Epoch: 28 [0/691 (0%)]\tAverage loss: 0.000157\n",
      "Train Epoch: 29 [0/691 (0%)]\tAverage loss: 0.000159\n",
      "Train Epoch: 30 [0/691 (0%)]\tAverage loss: 0.000243\n",
      "Train Epoch: 31 [0/691 (0%)]\tAverage loss: 0.000166\n",
      "Train Epoch: 32 [0/691 (0%)]\tAverage loss: 0.000576\n",
      "Train Epoch: 33 [0/691 (0%)]\tAverage loss: 0.000198\n",
      "Train Epoch: 34 [0/691 (0%)]\tAverage loss: 0.000567\n",
      "Train Epoch: 35 [0/691 (0%)]\tAverage loss: 0.000130\n",
      "Train Epoch: 36 [0/691 (0%)]\tAverage loss: 0.000156\n",
      "Train Epoch: 37 [0/691 (0%)]\tAverage loss: 0.000073\n",
      "Train Epoch: 38 [0/691 (0%)]\tAverage loss: 0.000204\n",
      "Train Epoch: 39 [0/691 (0%)]\tAverage loss: 0.000129\n",
      "Train Epoch: 40 [0/691 (0%)]\tAverage loss: 0.000104\n",
      "Train Epoch: 41 [0/691 (0%)]\tAverage loss: 0.000155\n",
      "Train Epoch: 42 [0/691 (0%)]\tAverage loss: 0.000379\n",
      "Train Epoch: 43 [0/691 (0%)]\tAverage loss: 0.000065\n",
      "Train Epoch: 44 [0/691 (0%)]\tAverage loss: 0.000141\n",
      "Train Epoch: 45 [0/691 (0%)]\tAverage loss: 0.000179\n",
      "Train Epoch: 46 [0/691 (0%)]\tAverage loss: 0.000047\n",
      "Train Epoch: 47 [0/691 (0%)]\tAverage loss: 0.000049\n",
      "Train Epoch: 48 [0/691 (0%)]\tAverage loss: 0.000041\n",
      "Train Epoch: 49 [0/691 (0%)]\tAverage loss: 0.000067\n",
      "Train Epoch: 50 [0/691 (0%)]\tAverage loss: 0.000042\n"
     ]
    }
   ],
   "source": [
    "_iter = 1\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    _iter = train(epoch, _iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0156, Accuracy: 156/174 (90%)\n",
      "\n",
      "['bird', 'cat', 'chicken', 'cow', 'dog', 'donkey', 'frog', 'lion', 'monkey', 'sheep']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bird       0.98      1.00      0.99        40\n",
      "         cat       0.90      0.95      0.93        40\n",
      "     chicken       0.67      0.67      0.67         6\n",
      "         cow       0.93      0.93      0.93        15\n",
      "         dog       0.92      0.87      0.89        39\n",
      "      donkey       0.83      1.00      0.91         5\n",
      "        frog       1.00      0.71      0.83         7\n",
      "        lion       0.57      0.89      0.70         9\n",
      "      monkey       1.00      0.80      0.89         5\n",
      "       sheep       1.00      0.50      0.67         8\n",
      "\n",
      "    accuracy                           0.90       174\n",
      "   macro avg       0.88      0.83      0.84       174\n",
      "weighted avg       0.91      0.90      0.90       174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = date.today().strftime(\"%Y_%m_%d\")\n",
    "if not os.path.exists('./models'):\n",
    "  os.makedirs('./models')\n",
    "torch.save({\n",
    "            'epoch': NUM_EPOCHS,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }, f\"./models/model_{date}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
